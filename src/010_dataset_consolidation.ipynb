{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734dcc24",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "conda install pip jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2f45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets > /dev/null  # HuggingFace framework\n",
    "!pip install lxml > /dev/null    # HTML parser and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31af0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentation = \"\"\"\n",
    "+ All data are assumed to be stored in a directory whose contents are:\n",
    "  + categories.tsv: Level-1 categories (ignored at the moment)\n",
    "  + food_976759.json: List of Level3 (and Level4, e.g. 976759_976794_7981173_5580286)\n",
    "    categories)\n",
    "    + features: \"name\", \"id\", \"itemCount\"\n",
    "  + data: (directory) contains (JSON) dictionaries of pages of products\n",
    "    + category_id = key in each dictionary\n",
    "    + category_id has the form <level1 category>_<level2 category>_<level3 category>...\n",
    "    + pages indexed by integers: \"1\", \"2\", ...\n",
    "    + products indexed by key, e.g. \"4PR4FBQD9P50\" (=product_id)\n",
    "    + We're interested in \"shortDescription\"\n",
    "\"\"\"\n",
    "\n",
    "notes = \"\"\"\n",
    "+ Each product belongs to a hierarchy of categories\n",
    "  (TBC: each product seems to belong to exactly one hierarchy of categories)\n",
    "+ Categories:\n",
    "  + Each category has its own numeric id, e.g.:\n",
    "    + Food is 976759\n",
    "    + Pantry is 976794 but its id is 976759_976794 (it's a subcategory of food)\n",
    "    + Condiments is 7981173 (id: 976759_976794_7981173)\n",
    "    + Salad Dressings & Toppings is 5580286 (id: 976759_976794_7981173_5580286)\n",
    "\n",
    "+ Data set is in $HOME/MachineLearning/mahdi/data-20221013\n",
    "\n",
    "Notes:\n",
    "+ categories.tsv is unlikely to change. We ignore it for the moment\n",
    "+ Two types: products and level_n categories (with n >= 1)\n",
    "+ Don't need pagination in product files\n",
    "+ Minimize operations on original data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eb7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import lxml.html as html\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "ds.utils.logging.set_verbosity(logging.ERROR)\n",
    "ds.utils.logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d7796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_html(string):\n",
    "    \"\"\"\n",
    "    Remove all HTML elements from string\n",
    "    \"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    _html_list = html.fragments_fromstring(string)\n",
    "    _str_list = []\n",
    "    for e in _html_list:\n",
    "        try:\n",
    "            if type(e) is str:\n",
    "                _str_list.append(e)\n",
    "            else:\n",
    "                if e.text is not None: # Sometimes there is pure-HTML with no text like <div>...</div>\n",
    "                    _str_list.append(e.text)\n",
    "        except TypeError as e:\n",
    "            logger.error(\"Type %s not supported\", type(e))\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "    return \" \".join(_str_list)\n",
    "\n",
    "\n",
    "def dig(json_object, category_id = None):\n",
    "    \"\"\"\n",
    "    We want to dig into the original data. They initially look like:\n",
    "    { <cat_id>: 1: {product1, product2, product3...}, 2: {product10, product11, product12...}, ... }\n",
    "    What we want is a list of all products and we want to add the cat_id in each product\n",
    "    \n",
    "    We explore (=recurrence) the json object. The cat_id is not known until the first level\n",
    "    is explored (and there is only one cat_id so it's easy)\n",
    "    Then we explore the page-level (1, 2, .. indices) to find each product\n",
    "    \"\"\"\n",
    "    if \"id\" in json_object:\n",
    "        # We know that it is a product since it has an \"id\" data member\n",
    "        # Add the category_id to the object\n",
    "        json_object[\"category_id\"] = category_id\n",
    "        # Scrub HTML from shortDescription \n",
    "        json_object[\"shortDescription\"] = scrub_html(json_object[\"shortDescription\"])\n",
    "        # ... and return it as is, i.e. return a object\n",
    "        # which has the JSON object type (this matters)\n",
    "        return json_object\n",
    "    else:\n",
    "        # We are not at the product json-object level... It is one of the levels above\n",
    "        if category_id is None:\n",
    "            # category_id is not known which means that it is the first level\n",
    "            # There is only one key which is the category_id\n",
    "            category_id = next(iter(json_object.keys()))\n",
    "        # results is a list where the products are collected\n",
    "        results = []\n",
    "        for value in json_object.values():  # We don't care about the keys here\n",
    "            # We are interested in the values of the current level, i.e.\n",
    "            # the pages (if at the top level) or the json-objects (if at the page one)\n",
    "            result = dig(value, category_id)\n",
    "            # We've returned from the inspection at lower level\n",
    "            # There are only two possibilities:\n",
    "            if type(result) is list:\n",
    "                # We've returned from a page or a level-3 exploration, i.e.\n",
    "                # we have a list of JSON objects and we want to add them all\n",
    "                # to the results\n",
    "                results.extend(result)\n",
    "            else:\n",
    "                # We've returned from a JSON object (i.e. not a list) and we\n",
    "                # just want to add that object to the list of results\n",
    "                results.append(result)\n",
    "        # Return the list of JSON objects collected by the exploration of this level\n",
    "        return results\n",
    "\n",
    "\n",
    "NO_NEED_TO_TELL_ME_TWICE = set()  # This is just to avoid having the same warnings over and over\n",
    "def consolidate_keys(products):\n",
    "    \"\"\"\n",
    "    Here we just want to make sure that all products (JSON objects)\n",
    "    have the same keys. We add None if a key is missing.\n",
    "    \"\"\"\n",
    "    # Collect all keys for all products\n",
    "    keys = set()\n",
    "    for product in products:\n",
    "        keys.update(product.keys())\n",
    "    logger.debug(\"All keys = %s\", keys)\n",
    "    # Add missing keys to each product\n",
    "    for product in products:\n",
    "        for key in keys:\n",
    "            if not key in product.keys():\n",
    "                if not key in NO_NEED_TO_TELL_ME_TWICE:\n",
    "                    logger.warning(\"Key %s is missing in %s\", key, product[\"id\"])\n",
    "                    NO_NEED_TO_TELL_ME_TWICE.add(key)\n",
    "                product[key] = None\n",
    "    return products\n",
    "\n",
    "\n",
    "def flatten(product, prefix = \"\"):\n",
    "    \"\"\"The products contain JSON objects. Similarly to the huggingface\n",
    "    flatten() method, we flatten them, i.e. if they look like:\n",
    "    { \"k1\": \"v1\", \"k2\": { \"k21\": \"v21\", \"k22\": \"v22\" } }\n",
    "    they become:\n",
    "    { \"k1\": \"v1\", \"k2_k21\": \"v21\", \"k2_k22\": \"v22\" }\n",
    "    \"\"\"\n",
    "    if prefix != \"\":\n",
    "        logger.debug(\"Flattening for prefix\", prefix)\n",
    "    n_product = {}\n",
    "    for key,value in product.items():\n",
    "        if not type(value) is dict:\n",
    "            # the value is a primitive JSON type\n",
    "            n_product[prefix + key] = value\n",
    "        else:\n",
    "            # the value is a Python-dictionary = JSON object \n",
    "            subdictionary = flatten(value, prefix = prefix + key + \".\")\n",
    "            n_product.update(subdictionary)\n",
    "    return n_product\n",
    "\n",
    "\n",
    "def process_arguments(arguments):\n",
    "    \"\"\"\n",
    "    Process the arguments on the command line\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Consolidate data into a single dataset.\n",
    "\n",
    "%s\n",
    "\n",
    "\"\"\" % documentation)\n",
    "    parser.add_argument('path_to_data', type=str,\n",
    "                        help=\"The directory containing data\")\n",
    "    parser.add_argument('-d', '--debug', required=False, default=False,\n",
    "                        help=\"Debugging message\")\n",
    "    args = parser.parse_args(arguments)\n",
    "    loglevel = logging.DEBUG if args.debug else logging.INFO\n",
    "    logging.basicConfig(format='%(asctime)s.%(msecs)03d | %(levelname)7s | %(message)s',\n",
    "                        datefmt=\"%Y-%m-%dT%H:%M:%S\",\n",
    "                        level=loglevel)\n",
    "    logger.info(\"Logging with level %s\", loglevel)\n",
    "    return args.path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edf9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arguments):\n",
    "    path_to_data = process_arguments(arguments[1:])\n",
    "    # path_to_data = \"/home/sc/MachineLearning/mahdi/data-20221013\"\n",
    "\n",
    "    # food_category = ds.load_dataset(\"json\", data_files = \"%s/%s\" %\n",
    "    #                                 (path_to_data, \"food_976759.json\"))\n",
    "    # Not sure what to do with this: Do we care?\n",
    "    \n",
    "    product_files = [\"%s/data/%s\" % (path_to_data, filename) for filename\n",
    "                     in os.listdir(\"%s/%s\" % (path_to_data, \"data\"))]\n",
    "    logger.info(\"There are %d product files to parse\" % len(product_files))\n",
    "\n",
    "    # Notes:\n",
    "    # + ds.load_dataset(\"json\", data_files=product_files) doesn't work\n",
    "    #   \"because column names don't match\"\n",
    "    # + ds.load_dataset(\"json\", data_files=product_file) for product_file in product_files\n",
    "    #   is awfully slow (if not parallelized)\n",
    "    # + going thru pandas.read_json() didn't help or speed up\n",
    "    products = []\n",
    "    for product_file in product_files:  # This could be parallelized if needed\n",
    "        logger.debug(\"Reading %s\", product_file)\n",
    "        with open(product_file) as pf:\n",
    "            json_product = json.load(pf)\n",
    "            products.extend(dig(json_product))\n",
    "    # Make sure all products have the same keys\n",
    "    products = consolidate_keys(products)\n",
    "    # Flatten the products. TODO Parallelize? \n",
    "    flattened_products = [flatten(product) for product in products]\n",
    "    # Write a JSON file containing all these entries\n",
    "    json_outfile = \"tmp_all_data.json\"\n",
    "    with open(json_outfile, \"w\") as out:\n",
    "        json.dump(flattened_products, out)\n",
    "\n",
    "    # Read it as a Huggingface-dataset\n",
    "    products = ds.load_dataset(\"json\", data_files = json_outfile)\n",
    "    # And dump it\n",
    "    print(products)\n",
    "    # Saving is idempotent but for the train/state.json fingerprint element\n",
    "    outdir = \"prepared-%s\" % path_to_data.split('/')[-1]\n",
    "    logger.info(\"Writing output huggingface dataset to %s\", outdir)\n",
    "    products.save_to_disk(outdir)\n",
    "    os.unlink(json_outfile)\n",
    "    return outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc49cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31T08:42:13.923 |    INFO | Logging with level 20\n",
      "2022-10-31T08:42:13.927 |    INFO | There are 407 product files to parse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/sc/.cache/huggingface/datasets/json/default-2944be280ba8b531/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31T08:42:22.901 |    INFO | Writing output huggingface dataset to prepared-data-20221013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/sc/.cache/huggingface/datasets/json/default-2944be280ba8b531/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['__typename', 'id', 'usItemId', 'fitmentLabel', 'name', 'checkStoreAvailabilityATC', 'seeShippingEligibility', 'brand', 'type', 'shortDescription', 'weightIncrement', 'imageInfo.thumbnailUrl', 'imageInfo.size', 'canonicalUrl', 'externalInfo', 'itemType', 'category.path', 'badges.flags', 'badges.tags', 'classType', 'averageRating', 'numberOfReviews', 'esrb', 'mediaRating', 'salesUnitType', 'sellerId', 'sellerName', 'hasSellerBadge', 'isEarlyAccessItem', 'earlyAccessEvent', 'annualEvent', 'availabilityStatusV2.display', 'availabilityStatusV2.value', 'groupMetaData.groupType', 'groupMetaData.groupSubType', 'groupMetaData.numberOfComponents', 'groupMetaData.groupComponents', 'productLocation', 'fulfillmentSpeed', 'offerId', 'preOrder.isPreOrder', 'preOrder.preOrderMessage', 'preOrder.preOrderStreetDateMessage', 'pac', 'priceInfo.priceRange', 'priceInfo.currentPrice.price', 'priceInfo.currentPrice.priceString', 'priceInfo.currentPrice.variantPriceString', 'priceInfo.currentPrice.priceType', 'priceInfo.currentPrice.currencyUnit', 'priceInfo.currentPrice.priceDisplay', 'priceInfo.comparisonPrice', 'priceInfo.wasPrice', 'priceInfo.unitPrice', 'priceInfo.listPrice', 'priceInfo.savingsAmount', 'priceInfo.shipPrice.price', 'priceInfo.shipPrice.priceString', 'priceInfo.shipPrice.variantPriceString', 'priceInfo.shipPrice.priceType', 'priceInfo.shipPrice.currencyUnit', 'priceInfo.shipPrice.priceDisplay', 'priceInfo.subscriptionPrice', 'priceInfo.priceDisplayCodes.priceDisplayCondition', 'priceInfo.priceDisplayCodes.finalCostByWeight', 'priceInfo.priceDisplayCodes.submapType', 'priceInfo.wPlusEarlyAccessPrice', 'variantCriteria', 'snapEligible', 'fulfillmentBadge', 'fulfillmentTitle', 'fulfillmentType', 'manufacturerName', 'showAtc', 'sponsoredProduct', 'showOptions', 'showBuyNow', 'quickShop', 'rewards', 'arExperiences.isARHome', 'arExperiences.isZeekit', 'eventAttributes.priceFlip', 'eventAttributes.specialBuy', 'subscription.subscriptionEligible', 'category_id'],\n",
      "        num_rows: 50243\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# main([\"\", \"/home/sc/MachineLearning/mahdi/data-20221013\", \"--debug\"])\n",
    "outdir = main([\"\", \"/home/sc/MachineLearning/mahdi/data-20221013\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6837cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['__typename', 'id', 'usItemId', 'fitmentLabel', 'name', 'checkStoreAvailabilityATC', 'seeShippingEligibility', 'brand', 'type', 'shortDescription', 'weightIncrement', 'imageInfo.thumbnailUrl', 'imageInfo.size', 'canonicalUrl', 'externalInfo', 'itemType', 'category.path', 'badges.flags', 'badges.tags', 'classType', 'averageRating', 'numberOfReviews', 'esrb', 'mediaRating', 'salesUnitType', 'sellerId', 'sellerName', 'hasSellerBadge', 'isEarlyAccessItem', 'earlyAccessEvent', 'annualEvent', 'availabilityStatusV2.display', 'availabilityStatusV2.value', 'groupMetaData.groupType', 'groupMetaData.groupSubType', 'groupMetaData.numberOfComponents', 'groupMetaData.groupComponents', 'productLocation', 'fulfillmentSpeed', 'offerId', 'preOrder.isPreOrder', 'preOrder.preOrderMessage', 'preOrder.preOrderStreetDateMessage', 'pac', 'priceInfo.priceRange', 'priceInfo.currentPrice.price', 'priceInfo.currentPrice.priceString', 'priceInfo.currentPrice.variantPriceString', 'priceInfo.currentPrice.priceType', 'priceInfo.currentPrice.currencyUnit', 'priceInfo.currentPrice.priceDisplay', 'priceInfo.comparisonPrice', 'priceInfo.wasPrice', 'priceInfo.unitPrice', 'priceInfo.listPrice', 'priceInfo.savingsAmount', 'priceInfo.shipPrice.price', 'priceInfo.shipPrice.priceString', 'priceInfo.shipPrice.variantPriceString', 'priceInfo.shipPrice.priceType', 'priceInfo.shipPrice.currencyUnit', 'priceInfo.shipPrice.priceDisplay', 'priceInfo.subscriptionPrice', 'priceInfo.priceDisplayCodes.priceDisplayCondition', 'priceInfo.priceDisplayCodes.finalCostByWeight', 'priceInfo.priceDisplayCodes.submapType', 'priceInfo.wPlusEarlyAccessPrice', 'variantCriteria', 'snapEligible', 'fulfillmentBadge', 'fulfillmentTitle', 'fulfillmentType', 'manufacturerName', 'showAtc', 'sponsoredProduct', 'showOptions', 'showBuyNow', 'quickShop', 'rewards', 'arExperiences.isARHome', 'arExperiences.isZeekit', 'eventAttributes.priceFlip', 'eventAttributes.specialBuy', 'subscription.subscriptionEligible', 'category_id'],\n",
       "        num_rows: 50243\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we can read the dataset\n",
    "dataset = ds.load_from_disk(outdir)\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
